\section{Projection and separation}

\begin{exercise}
  Give an example where the nearest point is unique, and one where it is not.
  Find a point $x$ and a set $S$ such that every point of $S$ is a nearest point to $x$!
\end{exercise}

\begin{solution}
  Consider first the singleton set $S = \{ 1 \} \subset \R$.
  Then, for any point $x \in \R$, the nearest point in $S$ to $x$ is simply $1$, which is unique.
  Now, consider the set $S = \{ -1, 1 \} \subset \R$.
  If we take $x = 0$, then both $-1$ and $1$ are nearest points to $x$, and thus the nearest point is not unique, and additionally every point in $S$ is a nearest point to $x$.
\end{solution}

\begin{exercise}
  Let $a \in \R^n \setminus \{0\}$ and $x_0 \in \R^n$.
  Then there is a unique hyperplane $H$ that contains $x_0$ and has a normal vector $a$.
  Verify this and find the value of the constant $\alpha$ in the definition of $H = \{ x \in \R^n : a^T x = \alpha \}$.
\end{exercise}

\begin{solution}
  The hyperplane $H$ is defined by $a^T x = a^T x_0$.
  Thus, the constant $\alpha$ is given by $\alpha = a^T x_0$.
  To see that this hyperplane is unique, suppose there is another hyperplane $H'$ with the same normal vector $a$ that also contains $x_0$.
  Then $H'$ must be defined by $a^T x = \beta$ for some constant $\beta$.
  Since $x_0 \in H'$, we have $a^T x_0 = \beta$.
  But we already have $a^T x_0 = \alpha$, so $\beta = \alpha$.
  Therefore, $H' = H$, proving the uniqueness of the hyperplane.
\end{solution}

\begin{exercise}
  Give an example of two disjoint sets $S$ and $T$ that cannot be separated by a hyperplane.
\end{exercise}

\begin{solution}
  Consider the sets $S = \{ x \in \R^2 : \norm{x}_2 < r \}$ and $T = \{ x \in \R^2 : r < \norm{x}_2 < 2r \}$ for some $r > 0$.
  These sets are disjoint, as there are no points that belong to both $S$ and $T$.
  However, they cannot be separated by a hyperplane, as there is no linear boundary that can separate the inner circle $S$ from the annular region $T$ without intersecting either set.
\end{solution}

\begin{exercise}
  In view of the previous remark, what about the separation of $S$ and a point $p \notin \aff(S)$?
  Is there an easy way to find a separating hyperplane?
\end{exercise}

\begin{solution}

\end{solution}

\begin{exercise}
  Let $C \subseteq \R^n$ be convex.
  Recall that if a point $x_0 \in C$ satisfies~\eqref{eq:unique_nearest} for any $y \in C$, then $x_0$ is the (unique) nearest point to $x$ in $C$.
  Now, let $C$ be the unit ball in $\R^n$ and let $x \in \R^n$ satisfy $\norm{x} > 1$.
  Find the nearest point to $x$ in $C$.
  What if $\norm{x} \leq 1$?
\end{exercise}

\begin{solution}
  \eqref{eq:unique_nearest} states % chktex 2
  \begin{equation}\label{eq:unique_nearest}
    (x - x_0)^T (y - x_0) \leq 0 \quad \text{for all } y \in C. \tag{3.2}
  \end{equation}
  When $\norm{x} > 1$, we have that $x \notin C$.
  The nearest point to $x$ in $C$ is given by
  \begin{equation}
    x_0 = \frac{x}{\norm{x}}.
  \end{equation}
  Plugging this into~\eqref{eq:unique_nearest}, we have for $\norm{x} > 1$ and any $y \in C$ that
  \begin{equation}
    \begin{split}
      (x - x_0)^T (y - x_0) &= \left(x - \frac{x}{\norm{x}}\right)^T \left(y - \frac{x}{\norm{x}}\right) \\
      &= \left(1 - \frac{1}{\norm{x}}\right) x^T \left(y - \frac{x}{\norm{x}}\right) \\
      &\leq x^T y - \frac{x^T x}{\norm{x}}
      = x^T y - \norm{x} \\
      &\leq \norm{x} \norm{y} - \norm{x} \\
      &\leq \norm{x} - \norm{x}
      = 0.
    \end{split}
  \end{equation}
  Therefore, $x_0$ is indeed the nearest point to $x$ in $C$ when $\norm{x} > 1$.

  For $\norm{x} \leq 1$, we have that $x \in C$, and thus the nearest point to $x$ in $C$ is simply $x$ itself.
\end{solution}

\begin{exercise}
  Let $L$ be a line in $\R^n$.
  Find the nearest point in $L$ to a point $x \in \R^n$.
  Use your result to find the nearest point on the line $L = \{ (x, y) : x + 3y = 5 \}$ to the point $(1, 2)$.
\end{exercise}

\begin{solution}
  Let $L$ be the defined by the line $\{ b + t d : t \in \R \}$, where $b$ is a point on the line and $d$ is a direction vector.
  The nearest point on $L$ to a point $x \in \R^n$ can be found by projecting the vector $x - b$ onto the direction vector $d$.
  The projection is given by
  \begin{equation}
    \mathrm{proj}_d (x - b) = b + \frac{(x - b)^T d}{\norm{d}^2} d.
  \end{equation}

  With $b = (5, 0)$, we note that $(1, 3)$ is orthogonal to the line, so we choose the direction vector to be $d = (-3, 1)$.
  Then, the nearest point on the line $L$ to the point $(1, 2)$ is given by
  \begin{equation}
    \begin{split}
      \mathrm{proj}_d ((1, 2) - (5, 0)) &= \mathrm{proj}_d ((-4, 2))
      = (5, 0) + \frac{(-4, 2)^T (-3, 1)}{\norm{(-3, 1)}^2} (-3, 1) \\
      &= (5, 0) + \frac{14}{10} (-3, 1)
      =\left(\frac{25}{5}, 0\right) + \left(-\frac{21}{5}, \frac{7}{5}\right) \\
      &= \left(\frac{4}{5}, \frac{7}{5}\right).
    \end{split}
  \end{equation}
  Therefore, the nearest point on the line $L$ to the point $(1, 2)$ is $\left(\frac{4}{5}, \frac{7}{5}\right)$.
\end{solution}

\begin{exercise}
  Let $H$ be a hyperplane in $\R^n$.
  Find the nearest point in $H$ to a point $x \in \R^n$.
  In particular, find the nearest point to each of the point $(0,0,0)$ and $(1,2,2)$ in the hyperplane $H = \{ (x_1, x_2, x_3) : x_1 + x_2 + x_3 = 1 \}$.
\end{exercise}

\begin{solution}
  The hyperplane $H$ can be defined by the equation $a^T x = \alpha$, where $a$ is the normal vector to the hyperplane and $\alpha$ is a constant.
  As we can write a hyperplane on the form $H = x_0 + L$ for some point $x_0 \in H$.
  Similarly as before, we can find the nearest point on $H$ to a point $x \in \R^n$ by projecting the vector $x - x_0$ onto the normal vector $a$.
  This gives us
  \begin{equation}
    \mathrm{proj}_H (x) = (x - x_0) - \frac{(x - x_0)^T a}{\norm{a}^2} a + x_0 = x - \frac{x^T a - \alpha}{\norm{a}^2} a,
  \end{equation}
  where the chosen point $x_0 \in H$ is arbitrary, as we can utilize the property $x_0^T a = \alpha$.
  For the hyperplane $H = \{ (x_1, x_2, x_3) : x_1 + x_2 + x_3 = 1 \}$, we can choose the normal vector $a = (1, 1, 1)$.
  Then, the nearest point on $H$ to the point $(0, 0, 0)$ is given by
  \begin{equation}
    \begin{split}
      \mathrm{proj}_H ((0, 0, 0)) &= (0, 0, 0) - \frac{(0, 0, 0)^T (1, 1, 1) - 1}{\norm{(1, 1, 1)}^2} (1, 1, 1) \\
      &= (0, 0, 0) - \frac{-1}{3} (1, 1, 1) \\
      &= \left(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right).
    \end{split}
  \end{equation}
  For the point $x = (1, 2, 2)$, we have $x^T a = 5$, so the nearest point is given by
  \begin{equation}
    \mathrm{proj}_H (x) = x - \frac{5 - 1}{3} a = (1, 2, 2) - \frac{4}{3} (1, 1, 1) = \left(-\frac{1}{3}, \frac{2}{3}, \frac{2}{3}\right).
  \end{equation}
\end{solution}

\begin{exercise}
  Let $L$ be a linear subspace in $\R^n$ and let $q_1, \ldots, q_t$ be an orthonormal basis for $L$.
  Thus $q_1, \ldots, q_t$ span $L$, and $q_i^T q_j = \delta_{ij}$.
  Let $Q$ be the $(n \times t)$-matrix whose $j$-th column is $q_j$.
  Define the associated matrix $P = Q Q^T$.
  Show that $Px$ is the nearest point in $L$ to $x$.
  (The matrix $P$ is called an orthogonal projector (or projection matrix)).
  Thus, performing the projection is simply to apply the linear transformation given by $P$.
  Let $L^\perp$ be the orthogonal complement of $L$.
  Explain why $(I - P)x$ is the nearest point in $L^\perp$ to $x$.
\end{exercise}

\begin{solution}
  Note firstly that we have $Q^T Q = I_t$, such that we have $P^2 = Q Q^T Q Q^T = Q I_t Q^T = P$, and thus $P$ is indeed a projection matrix.
  Consider then the point $b = Px$.
  We have
  \begin{equation}
    b = Px = P^2 x = P (Px) = P b.
  \end{equation}
  Note next that for a vector $y \in L$, we have $y = Q z$ for some $z \in \R^t$.
  Then,
  \begin{equation}
    (x - b)^T (y - b) = (x - Px)^T (Q z - Px) = x^T Q z - x^T P x - x^T P Q z + x^T P^2 x.
  \end{equation}
  Using the properties of $P$, we have $P Q = Q$ and $P^2 = P$, such that
  \begin{equation}
    (x - b)^T (y - b) = x^T Q z - x^T P x - x^T Q z + x^T P x = 0.
  \end{equation}
  Thus, $b = Px$ satisfies~\eqref{eq:unique_nearest}, and is therefore the nearest point in $L$ to $x$.

  For the orthogonal complement $L^\perp$, we have that any vector $w \in L^\perp$ satisfies $w^T v = 0$ for all $v \in L$.
  Note that for any $x \in \R^n$, we can decompose $x$ as $x = Px + (I - P)x$, where $Px \in L$.
  We have that $(I - P)x \in L^\perp$, since for any $v \in L$, we have
  \begin{equation}
    ((I - P)x)^T v = x^T (I - P)^T v = x^T (I - P) v = x^T (v - Pv) = x^T (v - v) = 0.
  \end{equation}
  Thus, $(I - P)x$ is orthogonal to every vector in $L$, and is therefore the nearest point in $L^\perp$ to $x$, as
  \begin{equation}
    \begin{split}
      (x - (I - P)x)^T (w - (I - P)x) &= (Px)^T (w - (I - P)x) \\
      &= (Px)^T w - x^T P (I - P)x \\
      &= 0 - x^T (P - P)x \\
      &= 0
    \end{split}
  \end{equation}
  for any $w \in L^\perp$, as $Px \in L$.
\end{solution}

\begin{exercise}
  Let $L \subset \R^3$ be a subspace spanned by the vectors $(1, 0, 1)$ and $(0, 1, 0)$.
  Find the nearest point to $(1, 2, 3)$ in $L$ using the results of the previous exercise.
\end{exercise}

\begin{solution}
  We then have
  \begin{equation}
    Q =
    \begin{bmatrix}
      \frac{1}{\sqrt{2}} & 0 \\
      0 & 1 \\
      \frac{1}{\sqrt{2}} & 0
    \end{bmatrix},
  \end{equation}
  such that $P$ is given by
  \begin{equation}
    P = Q Q^T =
    \begin{bmatrix}
      \frac{1}{\sqrt{2}} & 0 \\
      0 & 1 \\
      \frac{1}{\sqrt{2}} & 0
    \end{bmatrix}
    \begin{bmatrix}
      \frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\
      0 & 1 & 0
    \end{bmatrix} =
    \begin{bmatrix}
      \frac{1}{2} & 0 & \frac{1}{2} \\
      0 & 1 & 0 \\
      \frac{1}{2} & 0 & \frac{1}{2}
    \end{bmatrix}.
  \end{equation}
  The nearest point to $(1, 2, 3)$ in $L$ is then given by
  \begin{equation}
    P x =
    \begin{bmatrix}
      \frac{1}{2} & 0 & \frac{1}{2} \\
      0 & 1 & 0 \\
      \frac{1}{2} & 0 & \frac{1}{2}
    \end{bmatrix}
    \begin{bmatrix}
      1 \\
      2 \\
      3
    \end{bmatrix} =
    \begin{bmatrix}
      2 \\
      2 \\
      2
    \end{bmatrix}.
  \end{equation}
  Note: There is a mistake in the suggested solutions, as they haven't normalized the basis vectors.
\end{solution}

\begin{exercise}
  Show that the nearest point in $\R^n_+$ to $x \in \R^n$ is the point $x^+$ defined by $x^+_i = \max(x_i, 0)$.
\end{exercise}

\begin{solution}
  Let $C = \R^n_+$ and consider the point $x^+$ defined by $x^+_i = \max(x_i, 0)$.
  We want to show that $x^+$ is the nearest point in $C$ to $x$ by verifying~\eqref{eq:unique_nearest}.
  For any $y \in C$, we have
  \begin{equation}
    (x - x^+)^T (y - x^+) = \sum_{i=1}^n (x_i - x^+_i)(y_i - x^+_i).
  \end{equation}
  Note that if $x_i \geq 0$, then $x^+_i = x_i$, and thus the term $(x_i - x^+_i)(y_i - x^+_i) = 0$.
  If $x_i < 0$, then $x^+_i = 0$, and since $y_i \geq 0$ (as $y \in C$), we have $(x_i - x^+_i)(y_i - x^+_i) = x_i y_i \leq 0$.
  Therefore, each term in the sum is non-positive, and we conclude that
  \begin{equation}
    (x - x^+)^T (y - x^+) \leq 0,
  \end{equation}
  for all $y \in C$.
  Thus, by~\eqref{eq:unique_nearest}, $x^+$ is indeed the nearest point in $\R^n_+$ to $x$.
\end{solution}

\begin{exercise}
  Find a set $S \subset \R^n$ and a point $x \in \R^n$ with the property that every point of $S$ is nearest to $x$ in $S$!
\end{exercise}

\begin{solution}
  Let $S = \{ y \in \R^n : \norm{y} = r \}$ for some fixed $r > 0$, which is the surface of a sphere centred at the origin.
  Let $x = 0 \in \R^n$.
  Then, for any point $y \in S$, we have $\norm{x - y} = \norm{y} = r$.
  Since all points in $S$ are at the same distance $r$ from $x$, every point of $S$ is a nearest point to $x$ in $S$.
\end{solution}

\setexsol{13}

\begin{exercise}
  Let $C = [0,1] \times [0,1] \subset \R^2$ and let $y = (2,2)$.
  Find \emph{all} hyperplanes that separates $C$ and $y$.
\end{exercise}

\begin{solution}
  Let $H = \{ x \in \R^2 : a^T x = \alpha \}$ be a hyperplane that separates $C$ and $y$.
  Then, we must have either
  \begin{equation}
    \sup_{x \in C} a^T x < a^T y
    \quad\text{or}\quad
    \inf_{x \in C} a^T x > a^T y.
  \end{equation}
  We have $\sup_{x \in C} a^T x = \max(a_1, 0) + \max(a_2, 0)$ and $\inf_{x \in C} a^T x = \min(a_1, 0) + \min(a_2, 0)$.
  Thus, the first condition becomes
  \begin{equation}
    \max(a_1, 0) + \max(a_2, 0) < 2 (a_1 + a_2),
  \end{equation}
  and the second condition becomes
  \begin{equation}
    \min(a_1, 0) + \min(a_2, 0) > 2 (a_1 + a_2).
  \end{equation}

  There are several cases where this holds.
  Firstly, if $a \geq 0$, then we require $b \in [a_1 + a_2, 2 (a_1 + a_2))$. % chktex 9
  If $a_1 \geq 0 \geq a_2$, then we require $b \in [a_1, 2 (a_1 + a_2))$, and similarly for $a_2 \geq 0 \geq a_1$, we require $b \in [a_2, 2 (a_1 + a_2))$. % chktex 9
  If $a_1, a_2 \leq 0$, then we require $b \in (2(a_1 + a_2), a_1 + a_2]$. % chktex 9
\end{solution}

\begin{exercise}
  Let $C$ be a unit ball in $\R^n$ and let $y \notin C$.
  Find a hyperplane that separates $C$ and $y$.
\end{exercise}

\begin{solution}
  Choosing $x_0$ to be the midpoint between $y$ and the nearest point in $C$ to $y$, i.e.,
  \begin{equation}
    x_0 = \tfrac{1}{2} \left(y + \frac{y}{\norm{y}}\right),
  \end{equation}
  we can then simply choose $y$ to be the normal vector of the hyperplane.
  The hyperplane is then given by
  \begin{equation}
    H = \{ x \in \R^n : y^T x = x_0 \}.
  \end{equation}
\end{solution}

\begin{exercise}
  Find an example in $\R^2$ of two sets that have a unique separating hyperplane.
\end{exercise}

\begin{solution}
  Let $S = \{ (x, y) : x^2 + y^2 \leq 1, x \leq 0 \}$ and $T = \{ (x, y) : x^2 + y^2 \leq 1, x \geq 1 \}$.
  The unique separating hyperplane is then given by $H = \{ (x, y) : x = 0 \}$.
\end{solution}

\begin{exercise}
  Let $S,T \subseteq \R^n$.
  Explain the following fact: there exists a hyperplane that separates $S$ and $T$ if and only if there is a linear function $l: \R^n \to \R$ such that $l(s) \leq l(t)$ for all $s \in S$ and $t \in T$.
  Is there a similar equivalence for the notion of strong separation?
\end{exercise}

\begin{solution}
  A hyperplane that separates $S$ and $T$ can be defined by a normal vector $a$ and a constant $\alpha$ such that $a^T s \leq \alpha$ for all $s \in S$ and $a^T t \geq \alpha$ for all $t \in T$.
  Defining the linear function $l(x) = a^T x$, we have that $l(s) \leq \alpha \leq l(t)$ for all $s \in S$ and $t \in T$, which shows one direction of the equivalence.

  Conversely, if there exists a linear function $l: \R^n \to \R$ such that $l(s) \leq l(t)$ for all $s \in S$ and $t \in T$, we can express this linear function as $l(x) = a^T x$ for some vector $a$.
  Let $\alpha = \sup_{s \in S} l(s)$.
  Then, we have $a^T s \leq \alpha$ for all $s \in S$ and $a^T t \geq \alpha$ for all $t \in T$, which defines a hyperplane that separates $S$ and $T$.

  For strong separation, the equivalence holds with the additional requirement that there exists an $\varepsilon > 0$ such that $l(s) + \varepsilon < \alpha < l(t) - \varepsilon$ for all $s \in S$ and $t \in T$.
  This ensures that the hyperplane not only separates the sets but does so with a positive margin, thus achieving strong separation.
\end{solution}

\begin{exercise}
  Let $C$ be a non-empty closed convex set in $\R^n$.
  Then the associated projection operator $p_C$ is Lipschitz continuous with Lipschitz constant $1$, i.e.,
  \begin{equation}
    \norm*{p_C(x) - p_C(y)} \leq \norm{x - y} \quad \text{for all } x, y \in \R^n.
  \end{equation}
  (Such an operator is called non-expansive).
  You are asked to prove this using the following procedure:
  Define $a = x - p_C(x)$ and $b = y - p_C(y)$.
  Verify that $(a - b)^T (p_C(x) - p_C(y)) \geq 0$.
  (%
    Show first that $a^T (p_C(y) - p_C(x)) \leq 0$ and $b^T (p_C(x) - p_C(y)) \leq 0$ using~\eqref{eq:unique_nearest}.
    Then consider $\norm{x - y}^2 = \norm{(a - b) + (p_C(x) - p_C(y))}^2$ and do some calculations.%
  )
\end{exercise}

\begin{solution}
  Let's follow the helpful suggestions.
  Let $a = x - p_C(x)$ and $b = y - p_C(y)$.
  As $p_C(x)$ is the nearest point in $C$ to $x$, we have by~\eqref{eq:unique_nearest} that
  \begin{equation}
    (x - p_C(x))^T (z - p_C(x)) \leq 0 \quad \text{for all } z \in C,
  \end{equation}
  and therefore specifically for $z = p_C(y)$ as well.
  Similarly for $b$ and $p_C(y)$, we have
  \begin{equation}
    (p_C(y) - y)^T (p_C(y) - z) = (y - p_C(y))^T (z - p_C(y)) \leq 0 \quad \text{for all } z \in C,
  \end{equation}
  and thus specifically for $z = p_C(x)$.
  Adding these two inequalities, we get
  \begin{equation}
    (a - b)^T (p_C(y) - p_C(x)) = a^T (p_C(y) - p_C(x)) + b^T (p_C(x) - p_C(y)) \leq 0.
  \end{equation}
  This gives us
  \begin{equation}
    \begin{split}
      \norm{x - y}^2 &= \norm{(a - b) + (p_C(x) - p_C(y))}^2 \\
      &= \norm{a - b}^2 + \norm{p_C(x) - p_C(y)}^2 + 2 (a - b)^T (p_C(x) - p_C(y)) \\
      &\geq \norm{a - b}^2 + \norm{p_C(x) - p_C(y)}^2 \\
      &\geq \norm{p_C(x) - p_C(y)}^2,
    \end{split}
  \end{equation}
  as desired.
\end{solution}

\begin{exercise}
  Consider the outer description of closed convex sets given in Corollary~3.2.4.
  What is this description for each of the following sets:
  \begin{enumerate}[label = (\emph{\roman*})]
    \item $C_1 = \{ x \in \R^n : \norm{x} \leq 1 \}$,
    \item $C_2 = \conv(\{0, 1\}^n)$,
    \item $C_3$ is the convex hull of the points $(1,1)$, $(1,-1)$, $(-1,1)$, and $(-1,-1)$ in $\R^2$.
    \item $C_4$ is the convex hull of all vectors in $\R^n$ having components that are either $1$ or $-1$.
  \end{enumerate}
\end{exercise}

\begin{solution}
  I don't quite see the point of having almost identical subproblems, but alright.
  \begin{enumerate}[label = (\emph{\roman*})]
    \item The half-planes are the tangents to the unit ball, as used in a previous exercise.
    \item One half of the half-planes are given by $x_i = 0$ for $i = 1, \ldots, n$, and the other half by $x_i = 1$ for $i = 1, \ldots, n$.
    \item Similarly, here the half-planes are given by $x_1 = 1$, $x_1 = -1$, $x_2 = 1$, and $x_2 = -1$, almost equivalently to (\emph{ii}).
    \item Again, the half-planes are given by $x_i = 1$ and $x_i = -1$ for $i = 1, \ldots, n$, almost equivalently to (\emph{ii}) and (\emph{iii}).
  \end{enumerate}
\end{solution}